---
title: 从"AI能做什么"到"如何与AI协作"
date: 2026-02-01 20:09:32
tags: [AI辅助]
---

### 引言：那个熟悉的念头

在把一件事交给 AI 之前，我们心里往往有一个隐含期待："它应该能帮我省点事"

但实际操作中，流程通常是：先解释背景，再说明目标，中途补充"之前没写清楚但很重要"的约束，等 AI 给出结果后，开始修改、校对、推翻重来。做到一半，一个念头会自然浮现：

"这件事，我是不是自己来反而更快？"

如果你有过这种感觉，先别急着怀疑 AI，也别急着怀疑自己。更可能的问题是：我们一开始就在问一个不太对的问题

<!-- more -->

### 第一部分：为什么"选什么任务交给 AI"并不能真正解决问题

团队引入 AI 的早期，几乎所有讨论都会集中到同一个问题上：哪些任务适合交给 AI？

这个问题听起来理性且工程化，但很多团队很快会发现一个尴尬现实：同样的工具、同样的模型，有人觉得特别好用，有人觉得不过是"高级搜索"，效果极不稳定。

这通常不是因为"任务选错了"，而是因为——我们混淆了"工具适配"与"协作设计"这两种完全不同的逻辑

**1. 我们为什么会这样问？**

"选任务"的背后，是一套来自工具时代的直觉：AI 像一个能力不错但还不太靠谱的新人，那就先从低风险、边缘任务开始，只要任务选对了，效果自然就会好。

这套逻辑在工具时代成立。评估一个软件，我们确实会问：它适合干什么？

但 AI 并不是一把锤子或一把螺丝刀，它是一个需要理解上下文、参与推理过程的协作者。当我们用"工具适配"的逻辑去框定 AI，偏差就已经埋下了：我们默认任务本身是静态的、边界清晰的，只需要被"分配"给合适的人或机器

**2. "选任务"默认了三个并不成立的前提**

**前提一：任务本身是清晰、稳定、可直接执行的**

现实中很多"任务"，你心里很清楚：边界是模糊的，目标会在过程中调整，关键判断从未被显式写下。你能推进下去，靠的是经验、直觉和随时纠偏

把这种任务直接丢给 AI，再问"它适不适合"，答案几乎是否定的。但真正的问题是这个任务本来就没被整理成"可以协作"的形态，而非 AI 能力不足

**前提二：人类负责判断，AI 只负责执行**

在"选任务"的语境下，AI 被默认为放大执行力的工具：人类想清楚、拆解好，AI 来加速。这在简单场景下有效，但在复杂工作中很快失效

因为真正消耗人的，往往不是"执行"，而是反复对齐、结构化、发现哪里想得不够清楚——而这些，恰恰是 AI 可以参与甚至加速的部分。把 AI 限定为"执行层"，实际上浪费了这个协作者的价值

**前提三：AI 用不好，是提示词不够完美**

当结果不理想时，我们容易归因于提示词写得不行，或还没掌握"正确用法"。但在实践中，更常见的情况是：AI 已经把问题暴露出来了，只是我们没意识到那是问题

不是 AI 不会做，而是我们自己都说不清"什么才算做对"

**3. 如果问题不在任务选择，那真正的问题是什么？**

当我们换个角度，一个更底层的问题会浮现：我们的工作过程，本身是不是被设计成"可被协作"的？

如果一个工作：目标说不清、约束藏在脑子里、结果只能凭感觉判断、偏差只能在最后才发现——那不管协作者是 AI，还是一位刚入职的高手，表现都会不稳定

AI 并不是制造问题的角色，它只是把原本就存在的不确定性放大了。此时，再继续纠结"选什么任务"，已经很难再往前走了

我们需要的是另一套逻辑：不是"挑选任务分配给 AI"，而是"重新设计适合与 AI 协作的工作规范"



### 第二部分：为什么 AI 很难进入真实、严肃的工作

一个关键但经常被忽略的判断是：AI 表现不佳，通常不是模型问题，而是工作缺乏清晰、可验证的结构

注意，这里的"规范"并非第一部分批评的那种"基于直觉的任务分配流程"，而是指让隐性经验显性化、让主观判断可检查的基础设施

**1. 规范，在 AI 时代到底解决什么问题？**

很多人一提"规范"，下意识联想到流程、限制、官僚化。但在 AI 场景下，规范真正解决的是一件事：减少只能靠"猜"的空间

它做的事情很朴素：把"你心知但肚明"的背景写下来，把"到时候看情况"的标准变成可检查的条款。只有当这些东西存在，AI 才有可能稳定地产生可用结果。

**2. 为什么 AI 在"玩具项目"里显得特别聪明？**

写点小工具、生成创意草稿、探索初步想法时，AI 常常"随手生成"就让人觉得不错。因为这些任务：目标本来就不严格，很少有"绝对不能错"的硬性约束，质量高低主要靠主观感觉

在这种环境里，AI 的自由发挥反而是优势

**3. 为什么一到真实工作就不行了？**

真实工作恰恰相反：约束多且都重要，任意一个点没满足结果就不可用，成功标准往往只存在于资深者的经验中

在这里，AI 不再是"差不多就行"，而是必须"刚好符合"。这不是 AI 退化了，而是工作本身的真实复杂性暴露出来了

**4. AI 不擅长的，从来不是思考，而是"猜规则"**

人类很擅长在信息不完整时补全规则。你心里默认的一堆前提，可能从来没写出来，但你"知道它们应该被满足"。AI 不会猜，也不该猜

当它失败时，很多时候不是因为它"笨"，而是因为规则从未被明确过

**5. 让工作真正"适合 AI"的三个条件**

如果你想让 AI 稳定参与工作，可以用三个问题自检：

- 输入是不是可规范的？ 背景、目标、约束有没有被显式写清楚？
- 输出是不是可评估的？有没有明确的"可用/不可用"标准，而非纯主观感受？
- 过程是不是可拆解的？ 能不能在中途就发现偏差，而非等到最后验收？

这三点本质上都在问：这项工作有没有被认真设计过？

举个例子：假设你要让 AI 协助"撰写产品需求文档"

- 不可协作的版本："帮我写个需求文档，要包含登录功能，写得详细点。"（目标模糊，"详细"无法评估）
- 可协作的版本："撰写一份产品需求文档，目标用户是移动端新用户，核心约束包括：必须支持微信一键登录（因转化率数据支持），暂不支持密码登录（降低开发成本），需包含异常处理流程（网络中断/用户取消）。成功标准是：开发团队阅读后无需追问即可开始技术方案设计"

后者不一定保证 AI 写得完美，但提供了可验证的框架——你可以检查它是否遗漏了"异常处理"，是否明确了"不支持密码登录"的约束



### 结语：重新理解 AI 的位置，以及我们自己的

回到开头的那个念头："这事我自己来是不是更快？"

在旧有的工作方式下，这个念头往往是合理的——因为整理规范、明确标准、拆解过程本身就需要时间。但这恰恰揭示了一个被忽视的真相：许多我们认为"只能边做边想"的工作，其实从未被真正设计过

AI 的价值，从来不只是模型能力的问题。真正的分水岭是：我们是否愿意用更系统、更规范的方式，重新设计自己的工作

当工作本身变得清晰、可判断、可演进时，AI 才会从一个"偶尔好用的工具"，变成一个稳定、可信赖的协作者

更进一步说，未来的专业竞争力，可能正从"个人执行的速度"转向"设计协作规范的能力"——即如何为 AI（也为人类同事）构建一个低不确定性、高可验证性的工作环境

下次当你准备把任务交给 AI 时，不妨先花五分钟，试着写下那句你从未说出口的话："做到什么程度，才算真的完成了？"

这五分钟，很可能比接下来的五十分钟提示词调优更有价值



